{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BASIC CONCEPTS"
      ],
      "metadata": {
        "id": "QC2h2r32DsKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example Python code for a **basic perceptron** that takes two binary inputs (x1 and x2) and produces a binary output (y) based on a given threshold (Z) and weights (w1 and w2):"
      ],
      "metadata": {
        "id": "DU02X7DIhKra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znVc9kolg14d",
        "outputId": "51f28d44-7ff6-4cbb-cbe6-b74db5b2197c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "def perceptron(x1, x2, w1, w2, Z):\n",
        "    # Calculate the weighted sum\n",
        "    z = w1*x1 + w2*x2\n",
        "    \n",
        "    # Apply the activation function\n",
        "    if z > Z:\n",
        "        y = 1\n",
        "    else:\n",
        "        y = 0\n",
        "        \n",
        "    # Return the output\n",
        "    return y\n",
        "\n",
        "#you can call the function like this:\n",
        "y = perceptron(0, 1, 0.5, -0.5, 0)\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Here's an example Python code for an **AND perceptron** that takes two binary inputs (x1 and x2) and produces a binary output (y) based on a threshold of 0.5 and equal weights of 0.5 for both inputs:"
      ],
      "metadata": {
        "id": "P-p8ROauhxEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def and_perceptron(x1, x2):\n",
        "    # Set the weights and threshold for an AND perceptron\n",
        "    w1 = 0.5\n",
        "    w2 = 0.5\n",
        "    Z = 0.5\n",
        "    \n",
        "    # Calculate the weighted sum\n",
        "    z = w1*x1 + w2*x2\n",
        "    \n",
        "    # Apply the activation function\n",
        "    if z > Z:\n",
        "        y = 1\n",
        "    else:\n",
        "        y = 0\n",
        "        \n",
        "    # Return the output\n",
        "    return y\n",
        "\n",
        "\n",
        "y = and_perceptron(0, 0)\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkpOj3LfhjOj",
        "outputId": "d109f8d7-65ff-4297-d39d-6436044303d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **XOR function** cannot be implemented using a single-layer perceptron because it is not linearly separable. However, it can be implemented using a multi-layer perceptron (MLP) with at least one hidden layer.\n",
        "\n",
        "Here's an example Python code for an **XOR MLP** that takes two binary inputs (x1 and x2) and produces a binary output (y) using a hidden layer with two neurons and a sigmoid activation function:"
      ],
      "metadata": {
        "id": "HmcStIcgiECW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def xor_mlp(x1, x2):\n",
        "    # Set the weights and biases for the XOR MLP\n",
        "    w1 = np.array([[20, -20], [-20, 20]])\n",
        "    b1 = np.array([-10, 30])\n",
        "    w2 = np.array([[-20], [20]])\n",
        "    b2 = np.array([-30])\n",
        "    \n",
        "    # Calculate the hidden layer\n",
        "    z1 = np.dot(np.array([x1, x2]), w1) + b1\n",
        "    h1 = sigmoid(z1)\n",
        "    \n",
        "    # Calculate the output layer\n",
        "    z2 = np.dot(h1, w2) + b2\n",
        "    y = sigmoid(z2)\n",
        "    \n",
        "    # Round the output to 0 or 1\n",
        "    if y > 0.5:\n",
        "        y = 1\n",
        "    else:\n",
        "        y = 0\n",
        "    \n",
        "    # Return the output\n",
        "    return y\n",
        "\n",
        "\n",
        "    y = xor_mlp(0, 1)\n",
        "print(y)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoyKXjDoh7mR",
        "outputId": "0cfda5b2-bdc0-4a3a-97da-1671d67de041"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **perceptron** is a type of artificial neural network that can be used for binary classification problems. It takes a set of input signals, applies weights to those signals, and outputs a binary decision based on a threshold function. The perceptron learning algorithm adjusts the weights during training to improve the accuracy of the classification.\n",
        "\n",
        "Here's an example Python code for a **basic perceptron **that takes two binary inputs (x1 and x2) and produces a binary output (y) based on a given threshold (Z) and weights (w1 and w2):"
      ],
      "metadata": {
        "id": "THY6oVDLifvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron:\n",
        "    def __init__(self, w1, w2, Z):\n",
        "        self.w1 = w1\n",
        "        self.w2 = w2\n",
        "        self.Z = Z\n",
        "    \n",
        "    def predict(self, x1, x2):\n",
        "        # Calculate the weighted sum\n",
        "        z = self.w1*x1 + self.w2*x2\n",
        "        \n",
        "        # Apply the activation function\n",
        "        if z > self.Z:\n",
        "            y = 1\n",
        "        else:\n",
        "            y = 0\n",
        "            \n",
        "        # Return the output\n",
        "        return y\n",
        "    \n",
        "    def train(self, X, y, eta, epochs):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(X.shape[0]):\n",
        "                # Calculate the prediction and error\n",
        "                x1, x2 = X[i]\n",
        "                y_pred = self.predict(x1, x2)\n",
        "                error = y[i] - y_pred\n",
        "                \n",
        "                # Update the weights\n",
        "                self.w1 += eta * error * x1\n",
        "                self.w2 += eta * error * x2\n",
        "                self.Z += eta * error\n",
        "        \n",
        "        # Print the final weights and threshold\n",
        "        print('Final weights:', self.w1, self.w2)\n",
        "        print('Final threshold:', self.Z)\n",
        "\n",
        "\n",
        "\n",
        "##\n",
        "import numpy as np\n",
        "\n",
        "# Define the training data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Create a Perceptron object and train it\n",
        "p = Perceptron(0.1, 0.2, 0.3)\n",
        "p.train(X, y, 0.1, 10)\n",
        "\n",
        "# Test the trained perceptron\n",
        "for i in range(X.shape[0]):\n",
        "    x1, x2 = X[i]\n",
        "    y_pred = p.predict(x1, x2)\n",
        "    print(x1, x2, y_pred)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHi5g8xLidPQ",
        "outputId": "d0bf1059-59d1-4dd4-9913-def59cabd57a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights: 0.1 0.2\n",
            "Final threshold: 0.3\n",
            "0 0 0\n",
            "0 1 0\n",
            "1 0 0\n",
            "1 1 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example Python code that demonstrates how to train a simple neural network model with a fixed **number of epochs**:"
      ],
      "metadata": {
        "id": "67hg35cFjXPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train = X_train.reshape((60000, 784)) / 255.0\n",
        "X_test = X_test.reshape((10000, 784)) / 255.0\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Train the model with a fixed number of epochs\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the trained model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnjVZZzpi9-a",
        "outputId": "1a707403-f7fd-4793-9526-fa6bd8014399"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 12s 3ms/step - loss: 0.3046 - accuracy: 0.9155 - val_loss: 0.1670 - val_accuracy: 0.9523\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1458 - accuracy: 0.9576 - val_loss: 0.1245 - val_accuracy: 0.9630\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1064 - accuracy: 0.9686 - val_loss: 0.0997 - val_accuracy: 0.9693\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0849 - accuracy: 0.9747 - val_loss: 0.0963 - val_accuracy: 0.9725\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0699 - accuracy: 0.9783 - val_loss: 0.0927 - val_accuracy: 0.9727\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0588 - accuracy: 0.9822 - val_loss: 0.0853 - val_accuracy: 0.9731\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0493 - accuracy: 0.9849 - val_loss: 0.0880 - val_accuracy: 0.9734\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0427 - accuracy: 0.9867 - val_loss: 0.0850 - val_accuracy: 0.9746\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0355 - accuracy: 0.9888 - val_loss: 0.0856 - val_accuracy: 0.9741\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0322 - accuracy: 0.9901 - val_loss: 0.1054 - val_accuracy: 0.9730\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.1054 - accuracy: 0.9730\n",
            "Test accuracy: 0.9729999899864197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example Python code that calculates the weighted sum of a 2D input vector using a weight vector:\n",
        "**x = (x1, x2)> wh1 = (wh11, wh12)> so x>wh1 = wh11x1 + wh12x2**"
      ],
      "metadata": {
        "id": "dGiL0RoHli8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "wh11 = 11\n",
        "wh12 = 22\n",
        "\n",
        "# Define the input vector x and the weight vector wh1\n",
        "x = np.array([x1, x2])  # Replace x1 and x2 with actual values\n",
        "wh1 = np.array([wh11, wh12])  # Replace wh11 and wh12 with actual values\n",
        "\n",
        "# Calculate the weighted sum of x and wh1\n",
        "weighted_sum = np.dot(x, wh1)\n",
        "\n",
        "print(weighted_sum)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1oYTJTnj86W",
        "outputId": "047c1b8d-a360-46b0-cfd3-e811d3cd04ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear classifier** can be represented as a single layer perceptron \n",
        "y = f(w1*x1 + w2*x2 + ... + wn*xn + b)\n",
        "In this formula, y is the output (prediction) of the linear classifier, x1, x2, ..., xn are the input features, w1, w2, ..., wn are the weights for each feature, b is the bias term, and f is an activation function that maps the weighted sum to a final output value.\n",
        "\n",
        "For a linear classifier, the activation function f is usually a simple identity function or a step function that returns 1 if the weighted sum is greater than a threshold and 0 otherwise. In other words, the output of the linear classifier is a binary classification decision based on a linear combination of the input features.\n",
        "\n",
        "This formula can be implemented as a **single-layer perceptron** using the following code:\n"
      ],
      "metadata": {
        "id": "9P2VBHoKmR7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict(x, w, b):\n",
        "    \"\"\"Predict the output of a linear classifier.\"\"\"\n",
        "    z = np.dot(x, w) + b\n",
        "    y = np.where(z > 0, 1, 0)\n",
        "    return y\n",
        "\n"
      ],
      "metadata": {
        "id": "C4WG-oJHmKk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here's an example of how to create a **multi-layer perceptron (MLP) ** with a **variable number of perceptrons** in the hidden layer using the Keras API in Python:"
      ],
      "metadata": {
        "id": "FAr1Vw9rqoXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Define the number of input features and classes\n",
        "num_features = 10\n",
        "num_classes = 2\n",
        "\n",
        "# Define the number of perceptrons in the hidden layer\n",
        "num_hidden = 50\n",
        "\n",
        "# Create the MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(num_hidden, input_dim=num_features, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TTsmkFGmqbPR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation functions"
      ],
      "metadata": {
        "id": "M6xpawYgDlh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **step function** is a simple activation function that maps the input to either 0 or 1 based on a threshold. It has a discontinuous nature and can only produce binary outputs, making it unsuitable for many practical applications.\n",
        "\n",
        "The step function can be represented as:"
      ],
      "metadata": {
        "id": "LFp20C47r1I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step(x):\n",
        "    if x < 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n"
      ],
      "metadata": {
        "id": "P5gjSq8nry-g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **sigmoid function** is a common activation function used in neural networks. It has a characteristic S-shaped curve and can map any input to a value between 0 and 1, making it suitable for binary classification tasks.\n",
        "\n",
        "The sigmoid function can be represented as:"
      ],
      "metadata": {
        "id": "HQ1V7bN81SDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n"
      ],
      "metadata": {
        "id": "tETbNMjsr7vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **linear activation function** simply outputs the input value, scaled by a constant factor. It is a simple function that can be represented as:\n",
        "The **polynomial activation function** is a more complex function that can capture nonlinear relationships between the input and output of a neural network. It can be represented as:"
      ],
      "metadata": {
        "id": "3gwn3F1X1mYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "def polynomial(x, degree):\n",
        "    return x**degree\n"
      ],
      "metadata": {
        "id": "EruUuyaZ1d71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here is a general example of how to apply a **nonlinear transformation** to input data using the numpy library in Python:"
      ],
      "metadata": {
        "id": "M3YbjeO_2ep9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate some random input data\n",
        "x = np.random.rand(100, 5)  # 100 samples, 5 input variables\n",
        "\n",
        "# Apply a polynomial transformation of degree 2\n",
        "x_poly = np.hstack([x, x**2])\n",
        "\n",
        "# Apply a radial basis function (RBF) transformation\n",
        "centers = np.random.rand(10, 5)  # 10 centers, same dimension as input\n",
        "sigma = 0.1\n",
        "x_rbf = np.exp(-np.sum((x[:, None, :] - centers)**2, axis=2) / (2 * sigma**2))\n",
        "\n",
        "# Use the transformed data in a regression or classification model\n"
      ],
      "metadata": {
        "id": "tpnqWCeQ1_gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " here is a general example of how to fit a **projection pursuit regression (PPR)** model using the scikit-learn library in Python:"
      ],
      "metadata": {
        "id": "Nv7Z3oIg2yNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Generate some random input data\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 input variables\n",
        "y = np.random.rand(100)  # target variable\n",
        "\n",
        "# Fit a PPR model with M=2 projections using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_proj = pca.fit_transform(X)\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_proj, y)\n",
        "\n",
        "# Use the PPR model to predict new data\n",
        "X_new = np.random.rand(10)  # new input data\n",
        "X_new_proj = pca.transform(X_new.reshape(1, -1))\n",
        "y_pred = lr.predict(X_new_proj)\n",
        "\n",
        "# Alternatively, use a polynomial kernel to model quadratic terms\n",
        "from sklearn.kernel_approximation import PolynomialCountSketch\n",
        "X_poly = PolynomialCountSketch(degree=2).fit_transform(X)\n",
        "lr_poly = LinearRegression()\n",
        "lr_poly.fit(X_poly, y)\n",
        "y_pred_poly = lr_poly.predict(PolynomialCountSketch(degree=2).fit_transform(X_new.reshape(1, -1)))\n"
      ],
      "metadata": {
        "id": "sHkEfjcO2ntR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can use a **polynomial kernel approximation** to model quadratic terms directly, without projecting the data onto lower dimensions.\n",
        "Here's an example code using scikit-learn library to perform **Support Vector Regression with polynomial kernel**:"
      ],
      "metadata": {
        "id": "OovpAb813OTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "# generate sample data\n",
        "X = np.random.rand(100, 2)\n",
        "y = X[:, 0] ** 2 + X[:, 1] ** 2\n",
        "\n",
        "# create polynomial features up to degree 2\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# train SVR with polynomial kernel\n",
        "svr = SVR(kernel='poly', degree=2)\n",
        "svr.fit(X_poly, y)\n",
        "\n",
        "# predict on new data\n",
        "X_new = np.random.rand(10, 2)\n",
        "X_new_poly = poly.transform(X_new)\n",
        "y_pred = svr.predict(X_new_poly)\n"
      ],
      "metadata": {
        "id": "zH3FuDxq3CHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example code in Python using the **ReLU function** from the TensorFlow library:"
      ],
      "metadata": {
        "id": "PGKYyE0X34ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define input tensor\n",
        "x = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "\n",
        "# define ReLU activation function\n",
        "relu = tf.nn.relu(x)\n",
        "\n",
        "# create session and run computation\n",
        "with tf.Session() as sess:\n",
        "    # generate random input\n",
        "    input_data = np.random.randn(5, 10)\n",
        "    \n",
        "    # run computation with input\n",
        "    output = sess.run(relu, feed_dict={x: input_data})\n"
      ],
      "metadata": {
        "id": "F2l6j5623bJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the code to apply the **tanh function** and its derivative in Python:"
      ],
      "metadata": {
        "id": "Eoh3kbgP-Ol0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n"
      ],
      "metadata": {
        "id": "qQXtfSl64Vij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example code for the **ReLU activation function and its derivative:**"
      ],
      "metadata": {
        "id": "WryupvyH_Jz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# ReLU activation function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of ReLU activation function\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n"
      ],
      "metadata": {
        "id": "mWe-V7Tb-goc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example code for the **Leaky ReLU activation** function and its derivative:"
      ],
      "metadata": {
        "id": "9m0w7RJOADuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Leaky ReLU activation function\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.maximum(alpha*x, x)\n",
        "\n",
        "# Derivative of Leaky ReLU activation function\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    return np.where(x > 0, 1, alpha)\n"
      ],
      "metadata": {
        "id": "onC8ctIO_Qfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example code for the **Parametric ReLU activation function** and its derivative:"
      ],
      "metadata": {
        "id": "uNVM1P0YA8LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PReLU:\n",
        "    def __init__(self, alpha=0.01):\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.mask = x > 0\n",
        "        self.alpha_x = self.alpha * x\n",
        "        return np.where(self.mask, x, self.alpha_x)\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = np.where(self.mask, dout, dout * self.alpha)\n",
        "        dalpha = np.sum(dout * self.alpha_x * (1 - self.mask))\n",
        "        self.alpha -= dalpha * 0.001  # learning rate for alpha\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "C49A0MZcAUq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's the Python code for implementing the **ELU activation function:**"
      ],
      "metadata": {
        "id": "US0B8nbHBdkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def elu(x, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the Exponential Linear Unit (ELU) activation function.\n",
        "    \n",
        "    Args:\n",
        "        x (float or numpy array): Input to the activation function.\n",
        "        alpha (float): Slope of the negative region. Default is 1.0.\n",
        "        \n",
        "    Returns:\n",
        "        float or numpy array: Output of the activation function.\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, x, alpha*(np.exp(x)-1))\n",
        "\n",
        "\n",
        "def elu_derivative(x, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the derivative of the Exponential Linear Unit (ELU) activation function.\n",
        "    \n",
        "    Args:\n",
        "        x (float or numpy array): Input to the activation function.\n",
        "        alpha (float): Slope of the negative region. Default is 1.0.\n",
        "        \n",
        "    Returns:\n",
        "        float or numpy array: Derivative of the activation function.\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, 1, alpha*np.exp(x))\n"
      ],
      "metadata": {
        "id": "2rhnOlgjBEtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax Function**"
      ],
      "metadata": {
        "id": "Zf0jjVCJCKA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z)\n"
      ],
      "metadata": {
        "id": "Qe_YAtJSCJEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Optamization methods"
      ],
      "metadata": {
        "id": "giW55jDPDcla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent method**"
      ],
      "metadata": {
        "id": "leHUWxJADTyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the cost function to optimize\n",
        "def cost_function(X, y, theta):\n",
        "    m = len(y)  # Number of training examples\n",
        "    h = X @ theta  # Hypothesis function\n",
        "    J = (1 / (2 * m)) * np.sum((h - y) ** 2)  # Cost function\n",
        "    return J\n",
        "\n",
        "# Define the Gradient Descent function\n",
        "def gradient_descent(X, y, alpha, num_iterations):\n",
        "    m, n = X.shape  # Number of training examples and features\n",
        "    theta = np.zeros((n, 1))  # Initialize the parameters\n",
        "    J_history = np.zeros((num_iterations, 1))  # History of cost function values\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        h = X @ theta  # Hypothesis function\n",
        "        gradient = (1 / m) * X.T @ (h - y)  # Gradient of the cost function\n",
        "        theta = theta - alpha * gradient  # Update the parameters\n",
        "        J_history[i] = cost_function(X, y, theta)  # Store the cost function value\n",
        "        \n",
        "    return theta, J_history\n",
        "\n",
        "# Test the Gradient Descent function\n",
        "X = np.array([[1, 3], [1, 4], [1, 5], [1, 6]])  # Features matrix\n",
        "y = np.array([[1], [2], [3], [4]])  # Labels vector\n",
        "alpha = 0.1  # Learning rate\n",
        "num_iterations = 1000  # Number of iterations\n",
        "theta, J_history = gradient_descent(X, y, alpha, num_iterations)  # Run Gradient Descent\n",
        "\n",
        "# Print the learned parameters and the final cost function value\n",
        "print(\"Learned parameters:\")\n",
        "print(theta)\n",
        "print(\"Final cost function value:\")\n",
        "print(J_history[-1])\n"
      ],
      "metadata": {
        "id": "Z-eDejbhCUfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic Gradient Descent (SGD) algorithm:**"
      ],
      "metadata": {
        "id": "ZzHLLS2YErN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, learning_rate, epochs):\n",
        "    # initialize the weights\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    \n",
        "    # loop over the epochs\n",
        "    for epoch in range(epochs):\n",
        "        # shuffle the data for stochasticity\n",
        "        X, y = shuffle(X, y)\n",
        "        \n",
        "        # loop over each data point\n",
        "        for i in range(X.shape[0]):\n",
        "            # calculate the predicted value using the current weights\n",
        "            y_pred = sigmoid(np.dot(X[i], weights))\n",
        "            \n",
        "            # calculate the error and update the weights\n",
        "            error = y[i] - y_pred\n",
        "            weights += learning_rate * error * X[i]\n",
        "    \n",
        "    return weights\n"
      ],
      "metadata": {
        "id": "jOyN4IgdEqHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example code for **Mini-batch Gradient Descent**:"
      ],
      "metadata": {
        "id": "B_I_G0eWjf61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mini_batch_gradient_descent(X, y, alpha, epochs, batch_size):\n",
        "    # X: input data\n",
        "    # y: output data\n",
        "    # alpha: learning rate\n",
        "    # epochs: number of iterations to train the model\n",
        "    # batch_size: number of samples in each batch\n",
        "    \n",
        "    m = X.shape[0] # number of samples\n",
        "    n = X.shape[1] # number of features\n",
        "    num_batches = m // batch_size # number of mini-batches\n",
        "    \n",
        "    # initialize weights randomly\n",
        "    W = np.random.randn(n, 1)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # shuffle the data\n",
        "        permutation = np.random.permutation(m)\n",
        "        X_shuffled = X[permutation,:]\n",
        "        y_shuffled = y[permutation,:]\n",
        "        \n",
        "        # iterate over mini-batches\n",
        "        for i in range(num_batches):\n",
        "            start = i * batch_size\n",
        "            end = (i+1) * batch_size\n",
        "            X_batch = X_shuffled[start:end,:]\n",
        "            y_batch = y_shuffled[start:end,:]\n",
        "            \n",
        "            # compute gradient\n",
        "            error = X_batch.dot(W) - y_batch\n",
        "            gradient = (1/batch_size) * X_batch.T.dot(error)\n",
        "            \n",
        "            # update weights\n",
        "            W = W - alpha * gradient\n",
        "    \n",
        "    return W\n"
      ],
      "metadata": {
        "id": "qk2jA5bcEzb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **momentum method** is an extension of gradient descent that incorporates previous gradients to accelerate the convergence. Here is the code for the momentum method:"
      ],
      "metadata": {
        "id": "MnugrXkykVs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def momentum_gradient_descent(X, y, lr=0.01, beta=0.9, epochs=100):\n",
        "    # Initialize parameters\n",
        "    m, n = X.shape\n",
        "    w = np.zeros((n, 1))\n",
        "    v = np.zeros((n, 1))\n",
        "    loss_history = []\n",
        "    \n",
        "    # Loop over epochs\n",
        "    for i in range(epochs):\n",
        "        # Compute gradient\n",
        "        grad = np.dot(X.T, (np.dot(X, w) - y)) / m\n",
        "        \n",
        "        # Update velocity\n",
        "        v = beta * v + (1 - beta) * grad\n",
        "        \n",
        "        # Update parameters\n",
        "        w = w - lr * v\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = np.mean((np.dot(X, w) - y) ** 2)\n",
        "        loss_history.append(loss)\n",
        "        \n",
        "    return w, loss_history\n"
      ],
      "metadata": {
        "id": "Tjdw9ouFjskt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RMSProp (Root Mean Square Prop)** is an optimization algorithm used for training neural networks. It is an adaptive learning rate method that modifies the learning rate of each weight based on the root mean squared of the gradients. Here's the code for RMSProp:"
      ],
      "metadata": {
        "id": "Us1IBD3vlSdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmsprop(weights, gradients, lr, decay_rate, cache):\n",
        "    eps = 1e-8 # small constant to avoid division by zero\n",
        "    for i in range(len(weights)):\n",
        "        cache[i] = decay_rate * cache[i] + (1 - decay_rate) * gradients[i]**2\n",
        "        weights[i] -= lr * gradients[i] / (np.sqrt(cache[i]) + eps)\n",
        "    return weights, cache\n"
      ],
      "metadata": {
        "id": "RxlqHfw0kgUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam is an adaptive learning rate optimization algorithm that combines the advantages of both momentum method and RMSProp. Here is the code for **Adam optimizer**:"
      ],
      "metadata": {
        "id": "NQYB230ElouN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = 0\n",
        "        self.v = 0\n",
        "        self.t = 0\n",
        "    \n",
        "    def update(self, w, grad_wrt_w):\n",
        "        self.t += 1\n",
        "        self.m = self.beta1 * self.m + (1 - self.beta1) * grad_wrt_w\n",
        "        self.v = self.beta2 * self.v + (1 - self.beta2) * np.square(grad_wrt_w)\n",
        "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
        "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
        "        w -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "        return w\n"
      ],
      "metadata": {
        "id": "ucI92U5Ilb3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BACK PROPAGATION"
      ],
      "metadata": {
        "id": "7-u2Us0wm3lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backpropagation** is a supervised learning algorithm used for training artificial neural networks. It is based on the chain rule of calculus, and it allows the network to adjust its weights in order to minimize the difference between its output and the desired output.\n",
        "\n",
        "The backpropagation algorithm can be divided into two phases: forward propagation and backward propagation. In the forward propagation phase, the input is fed through the network and the output is calculated. In the backward propagation phase, the error is calculated and propagated back through the network to adjust the weights."
      ],
      "metadata": {
        "id": "JYdZEL4wm-2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a high-level pseudocode for the backpropagation algorithm:\n",
        "\n",
        "// **Feed-forward phase**\n",
        "\n",
        "For each layer in the network:\n",
        "    Calculate the output of each neuron in the layer based on the input and the weights\n",
        "    Store the output for later use in the backpropagation phase\n",
        "    \n",
        "// **Backward propagation phase**\n",
        "\n",
        "Calculate the error between the network's output and the desired output\n",
        "For each layer in the network (starting from the output layer and working backwards):\n",
        "    Calculate the error for each neuron in the layer based on the error from the next layer and the weights\n",
        "    Update the weights for each neuron in the layer based on the error and the input\n"
      ],
      "metadata": {
        "id": "opQK5x2wnGig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example code for **backpropagation** in a **two-layer neural network** using sigmoid activation function and mean squared error loss:"
      ],
      "metadata": {
        "id": "0yBVaeEanves"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the derivative of sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Initialize network weights and biases\n",
        "n_input = 2\n",
        "n_hidden = 3\n",
        "n_output = 1\n",
        "\n",
        "W1 = np.random.randn(n_hidden, n_input)\n",
        "b1 = np.zeros((n_hidden, 1))\n",
        "W2 = np.random.randn(n_output, n_hidden)\n",
        "b2 = np.zeros((n_output, 1))\n",
        "\n",
        "# Define input and output data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T\n",
        "Y = np.array([[0, 1, 1, 0]])\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Train the network using backpropagation\n",
        "for i in range(epochs):\n",
        "    # Forward pass\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    # Compute error\n",
        "    loss = np.mean((A2 - Y) ** 2)\n",
        "\n",
        "    # Backward pass\n",
        "    dA2 = 2 * (A2 - Y)\n",
        "    dZ2 = dA2 * sigmoid_derivative(Z2)\n",
        "    dW2 = np.dot(dZ2, A1.T)\n",
        "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
        "    dW1 = np.dot(dZ1, X.T)\n",
        "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    # Print the loss after every 1000 epochs\n",
        "    if i % 1000 == 0:\n",
        "        print(f\"Epoch {i}: loss = {loss}\")\n"
      ],
      "metadata": {
        "id": "1a9yrTYLlz7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}